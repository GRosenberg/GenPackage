delimiters "%", "%"
import "D:\DevFiles\Java\WorkSpaces\Main\GenProject\src\net\certiv\antlr\project\libs\Header.stg"

AbstractBaseClassCopyright() ::= <<
 * Copyright (c) 2008-2014 G Rosenberg.
 * 
>>

AbstractBaseClassContributor() ::= <<
 *		G Rosenberg - initial API and implementation
 * 
>>

AbstractBaseClassVersion() ::= <<
 * 		1.0 - 2014.03.26: First release level code
 * 		1.1 - 2014.08.26: Updates, add Tests support
 * 
>>

AbstractBaseClass(packageName, grammarName, startRule) ::= <<
%hdrBeg()%
%AbstractBaseClassCopyright()%
%epl()%
%hdrBlankLn()%
%hdrContribPrefix()%
%AbstractBaseClassContributor()%
%hdrBlankLn()%
%hdrVersionPrefix()%
%AbstractBaseClassVersion()%
%hdrBlankLn()%
%hdrEnd()%
package %packageName%.test.base;

import static org.junit.Assert.assertNotNull;
import static org.junit.Assert.assertTrue;

import java.io.File;
import java.util.ArrayList;
import java.util.HashSet;
import java.util.List;
import java.util.Set;
import java.util.StringTokenizer;

import %packageName%.parser.%grammarName%Token;
import %packageName%.parser.gen.%grammarName%Parser;
import %packageName%.parser.gen.%grammarName%ParserBaseListener;

import org.antlr.v4.runtime.ANTLRInputStream;
import org.antlr.v4.runtime.CharStream;
import org.antlr.v4.runtime.CommonTokenStream;
import org.antlr.v4.runtime.IntStream;
import org.antlr.v4.runtime.Lexer;
import org.antlr.v4.runtime.ParserRuleContext;
import org.antlr.v4.runtime.Token;
import org.antlr.v4.runtime.atn.ATN;
import org.antlr.v4.runtime.atn.LexerATNSimulator;
import org.antlr.v4.runtime.dfa.DFA;
import org.antlr.v4.runtime.tree.ParseTree;
import org.antlr.v4.tool.Grammar;
import org.antlr.v4.tool.LexerGrammar;
import org.junit.Assert;

public abstract class AbstractBase {

	public static final String TAB = "\\t";
	public static final String EOL = System.getProperty("line.separator");
	public static final String PSEP = System.getProperty("path.separator");

	public String tmpdir = null;

	/** reset during setUp and set to true if we find a problem */
	protected boolean lastTestFailed = false;

	public void setUpTempDir() throws Exception { // new output dir for each test
		lastTestFailed = false; // hope for the best, but set to true in asserts that fail
		long time = System.currentTimeMillis();
		tmpdir = new File("/tmp", getClass().getSimpleName() + "-" + time).getAbsolutePath();
		mkdir(tmpdir);
	\}

	public void tearDownTempDir() throws Exception { // remove tmpdir if no error.
		if (!lastTestFailed) eraseTempDir();
	\}

	public String lexSource(String source, boolean output, boolean style) {
		CommonTokenStream tokens = produceTokens(source);
		tokens.fill();
		StringBuilder sb = new StringBuilder();
		for (Token token : tokens.getTokens()) {
			((%grammarName%Token) token).toStringStyle(style);
			sb.append(token.toString());
			if (output) System.out.print(token.toString());
		\}
		return sb.toString();
	\}

	public CommonTokenStream produceTokens(String source) {
		ANTLRInputStream is = new ANTLRInputStream(source);
		return createLexerStream(is);
	\}

	public abstract CommonTokenStream createLexerStream(ANTLRInputStream is);

	public List<String> getTokenTypes(LexerGrammar lg, ATN atn, CharStream input) {
		LexerATNSimulator interp = new LexerATNSimulator(atn, new DFA[] { new DFA(
				atn.modeToStartState.get(Lexer.DEFAULT_MODE)) \}, null);
		List<String> tokenTypes = new ArrayList<String>();
		int ttype;
		boolean hitEOF = false;
		do {
			if (hitEOF) {
				tokenTypes.add("EOF");
				break;
			\}
			int t = input.LA(1);
			ttype = interp.match(input, Lexer.DEFAULT_MODE);
			if (ttype == Token.EOF) {
				tokenTypes.add("EOF");
			\}
			else {
				tokenTypes.add(lg.typeToTokenList.get(ttype));
			\}

			if (t == IntStream.EOF) {
				hitEOF = true;
			\}
		\} while (ttype != Token.EOF);
		return tokenTypes;
	\}

	public ParseTree produceTree(CommonTokenStream tokens) {
		%grammarName%Parser parser = new %grammarName%Parser(tokens);
		return parser.%startRule%();
	\}

	protected void checkSymbols(Grammar g, String rulesStr, String allValidTokensStr) throws Exception {
		String[] typeToTokenName = g.getTokenNames();
		Set<String> tokens = new HashSet<String>();
		for (int i = 0; i < typeToTokenName.length; i++) {
			String t = typeToTokenName[i];
			if (t != null) {
				if (t.startsWith(Grammar.AUTO_GENERATED_TOKEN_NAME_PREFIX)) {
					tokens.add(g.getTokenDisplayName(i));
				\}
				else {
					tokens.add(t);
				\}
			\}
		\}

		// make sure expected tokens are there
		StringTokenizer st = new StringTokenizer(allValidTokensStr, ", ");
		while (st.hasMoreTokens()) {
			String tokenName = st.nextToken();
			assertTrue("token " + tokenName + " expected, but was undefined",
					g.getTokenType(tokenName) != Token.INVALID_TYPE);
			tokens.remove(tokenName);
		\}
		// make sure there are not any others (other than <EOF> etc...)
		for (String tokenName : tokens) {
			assertTrue("unexpected token name " + tokenName,
					g.getTokenType(tokenName) < Token.MIN_USER_TOKEN_TYPE);
		\}

		// make sure all expected rules are there
		st = new StringTokenizer(rulesStr, ", ");
		int n = 0;
		while (st.hasMoreTokens()) {
			String ruleName = st.nextToken();
			assertNotNull("rule " + ruleName + " expected", g.getRule(ruleName));
			n++;
		\}
		Assert.assertEquals("number of rules mismatch; expecting " + n + "; found " + g.rules.size(), n, g.rules.size());
	\}

	public void assertEquals(Object expected, Object actual) {
		try {
			Assert.assertEquals(expected, actual);
		\} catch (Error e) {
			lastTestFailed = true;
			throw e;
		\}
	\}

	protected void mkdir(String dir) {
		File f = new File(dir);
		f.mkdirs();
	\}

	protected void eraseTempDir() {
		File tmpdirF = new File(tmpdir);
		if (tmpdirF.exists()) {
			eraseFiles();
			tmpdirF.delete();
		\}
	\}

	protected void eraseFiles() {
		if (tmpdir == null) return;
		File tmpdirF = new File(tmpdir);
		String[] files = tmpdirF.list();
		for (int i = 0; files != null && i < files.length; i++) {
			new File(tmpdir + PSEP + files[i]).delete();
		\}
	\}

	public class ProcessEveryRule extends %grammarName%ParserBaseListener {

		@Override
		public void enterEveryRule(ParserRuleContext ctx) {
			super.enterEveryRule(ctx);
		\}

		@Override
		public void exitEveryRule(ParserRuleContext ctx) {
			super.exitEveryRule(ctx);
		\}
	\}
\}


>>

TestBaseClassCopyright() ::= <<
 * Copyright (c) 2008-2014 G Rosenberg.
 * 
>>

TestBaseClassContributor() ::= <<
 *		G Rosenberg - initial API and implementation
 * 
>>

TestBaseClassVersion() ::= <<
 * 		1.0 - 2014.03.26: First release level code
 * 		1.1 - 2014.08.26: Updates, add Tests support
 * 
>>

TestBaseClass(packageName, grammarName) ::= <<
%hdrBeg()%
%TestBaseClassCopyright()%
%epl()%
%hdrBlankLn()%
%hdrContribPrefix()%
%TestBaseClassContributor()%
%hdrBlankLn()%
%hdrVersionPrefix()%
%TestBaseClassVersion()%
%hdrBlankLn()%
%hdrEnd()%
package %packageName%.test.base;

import %packageName%.parser.%grammarName%TokenFactory;
import %packageName%.parser.gen.%grammarName%Lexer;

import org.antlr.v4.runtime.ANTLRInputStream;
import org.antlr.v4.runtime.CommonTokenStream;

public class TestBase extends AbstractBase {

	// public String srcDir = "";
	// public String dstDir = srcDir;
	// public String nameIn = "";
	// public String namOut = "";

	@Override
	public CommonTokenStream createLexerStream(ANTLRInputStream is) {
		%grammarName%Lexer lexer = new %grammarName%Lexer(is);
		%grammarName%TokenFactory factory = new %grammarName%TokenFactory(is);
		lexer.setTokenFactory(factory);
		return new CommonTokenStream(lexer);
	\}
\}

>>

TestWordClassCopyright() ::= <<
 * Copyright (c) 2008-2014 G Rosenberg.
 * 
>>

TestWordClassContributor() ::= <<
 *		G Rosenberg - initial API and implementation
 * 
>>

TestWordClassVersion() ::= <<
 * 		1.0 - 2014.03.26: First release level code
 * 		1.1 - 2014.08.26: Updates, add Tests support
 * 
>>

TestWordClass(packageName, grammarName) ::= <<
%hdrBeg()%
%TestWordClassCopyright()%
%epl()%
%hdrBlankLn()%
%hdrContribPrefix()%
%TestWordClassContributor()%
%hdrBlankLn()%
%hdrVersionPrefix()%
%TestWordClassVersion()%
%hdrBlankLn()%
%hdrEnd()%
package %packageName%.test;

import %packageName%.IOProcessor;
import %packageName%.converter.Converter;
import %packageName%.parser.%grammarName%Token;
import %packageName%.test.base.TestBase;

import org.junit.After;
import org.junit.Before;
import org.junit.Test;

public class TestWord extends TestBase {

	private Converter converter;

	@Before
	public void setUp() throws Exception {
		String[] args = { "-s" \};
		IOProcessor processor = new IOProcessor(args);
		converter = new Converter(processor);
	\}

	@After
	public void tearDown() throws Exception {
		converter = null;
	\}

	@Test
	public void testLexBold() throws Exception {
		String source = "This is a *simple* sentence." + EOL;
		String found = lexSource(source, true, %grammarName%Token.BASIC);
		String expecting = "[@0   1:0  Word='This' ]" + EOL
				+ "[@1   1:4  Ws=' ' Hidden]" + EOL
				+ "[@2   1:5  Word='is' ]" + EOL
				+ "[@3   1:7  Ws=' ' Hidden]" + EOL
				+ "[@4   1:8  Word='a' ]" + EOL
				+ "[@5   1:9  Ws=' ' Hidden]" + EOL
				+ "[@6   1:10 Star='*' ]" + EOL
				+ "[@7   1:11 Word='simple' ]" + EOL
				+ "[@8   1:17 Star='*' ]" + EOL
				+ "[@9   1:18 Ws=' ' Hidden]" + EOL
				+ "[@10  1:19 Word='sentence.' ]" + EOL
				+ "[@11  1:28 Blankline='rn' ]" + EOL//
				+ "[@12  2:30 Eof='<EOF>' ]" + EOL;
		assertEquals(expecting, found);
	\}

	@Test
	public void testConvertBold() {
		String source = "This is a *simple* sentence." + EOL;
		String result = converter.convert(source);
		String expecting = "<p>This is a <b>simple</b> sentence.</p>" + EOL;
		assertEquals(expecting, result);
	\}

	@Test
	public void testLexUnderscore() throws Exception {
		String source = "Test: _underline_ text." + EOL;
		String found = lexSource(source, true, %grammarName%Token.BASIC);
		String expecting = "[@0   1:0  Word='Test:' ]" + EOL
				+ "[@1   1:5  Ws=' ' Hidden]" + EOL
				+ "[@2   1:6  Underscore='_' ]" + EOL
				+ "[@3   1:7  Word='underline' ]" + EOL
				+ "[@4   1:16 Underscore='_' ]" + EOL
				+ "[@5   1:17 Ws=' ' Hidden]" + EOL
				+ "[@6   1:18 Word='text.' ]" + EOL
				+ "[@7   1:23 Blankline='rn' ]" + EOL
				+ "[@8   2:25 Eof='<EOF>' ]" + EOL;
		assertEquals(expecting, found);
	\}

	@Test
	public void testWordPunct() throws Exception {
		String source = "And, just_a simple-test. Here & now: a *'special'* don't." + EOL;
		String found = lexSource(source, true, %grammarName%Token.BASIC);
		String expecting = "[@0   1:0  Word='And,' ]" + EOL
				+ "[@1   1:4  Ws=' ' Hidden]" + EOL
				+ "[@2   1:5  Word='just_a' ]" + EOL
				+ "[@3   1:11 Ws=' ' Hidden]" + EOL
				+ "[@4   1:12 Word='simple-test.' ]" + EOL
				+ "[@5   1:24 Ws=' ' Hidden]" + EOL
				+ "[@6   1:25 Word='Here' ]" + EOL
				+ "[@7   1:29 Ws=' ' Hidden]" + EOL
				+ "[@8   1:30 Word='&' ]" + EOL
				+ "[@9   1:31 Ws=' ' Hidden]" + EOL
				+ "[@10  1:32 Word='now:' ]" + EOL
				+ "[@11  1:36 Ws=' ' Hidden]" + EOL
				+ "[@12  1:37 Word='a' ]" + EOL
				+ "[@13  1:38 Ws=' ' Hidden]" + EOL
				+ "[@14  1:39 Star='*' ]" + EOL
				+ "[@15  1:40 SQuote=''' ]" + EOL
				+ "[@16  1:41 Word='special' ]" + EOL
				+ "[@17  1:48 SQuote=''' ]" + EOL
				+ "[@18  1:49 Star='*' ]" + EOL
				+ "[@19  1:50 Ws=' ' Hidden]" + EOL
				+ "[@20  1:51 Word='don't.' ]" + EOL
				+ "[@21  1:57 Blankline='rn' ]" + EOL
				+ "[@22  2:59 Eof='<EOF>' ]" + EOL;
		assertEquals(expecting, found);
	\}
\}


>>
